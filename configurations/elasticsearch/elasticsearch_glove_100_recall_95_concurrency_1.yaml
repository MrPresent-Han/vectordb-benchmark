# This configuration is for the concurrency test

database_params: {}
# The following parameters will be used to generate vectors for searching when you do not pass in search_vectors
# By default, parameters are automatically obtained from the given collection

# Connection parameters can be added dynamically to connect to elasticsearch
connection_params:
  max_retries: 100

indices_params:
# Connect to the specified collection in elasticsearch server, the default is elasticsearch_benchmark_index
  index: elasticsearch_benchmark_index

# The following are the parameters used in concurrent
concurrent_params:
  # Make sure the current machine resources are sufficient to start the number of processes
  concurrent_number: 1  # Total concurrent
  # This tool is suitable for performance testing, please keep the concurrency within a reasonable range
  # total concurrent time = warm_time * 2 + during_time
  # The state that includes warmup time and the state that does not include warmup time will be printed separately in the end state
  during_time: 60 # seconds
  warm_time: 0  # seconds
  interval: 20  # seconds, time interval for printing intermediate states
concurrent_tasks:
# Currently, one interface is supported concurrently
# `type`: supported interface type
# `weight`: when concurrent, this interface accounts for the weight of all interfaces
# `params`: interface parameters
  - type: search
    weight: 1
    params:
      # params for search param knn
      nq: 1  # only support nq equal to 1
      top_k: 100
      num_candidates: 188
#      filter:
#        range:
#          float:
#            gte: 1000

      timeout: '1m'
      search_vectors: "glove-100-angular.hdf5"

      # If you select the hdf5 dataset, the vector in the `test` set will be used as the search vectors
      # If you remove the search_vectors field, the vectors will be randomly generated
      # This field also supports vectors in formats: *.npy / *.json / *.txt
      # Please use the absolute path of the file
      # Unless you choose a file that already exists in the ./datasets/dataset_files/, you can directly write the file name
